{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introduction to Linear Regression\n","Linear regression is a statistical method used to model the relationship between a dependent variable (target) and one or more independent variables (features). It is widely used for prediction and forecasting in various fields such as economics, finance, and science.\n","\n","## 1. What is Linear Regression?\n","Linear regression assumes that there is a linear relationship between the independent variable(s) and the dependent variable and that this relationship can be represented by a straight line (or hyperplane). Linear regression aims to find the best-fitting linear equation that describes this relationship. \n","\n","### Applications:\n","- **Predicting Exam Scores:** Imagine you have data on students' study hours and their corresponding exam scores. You can use linear regression to predict a student's exam score based on the number of hours they studied.\n","\n","- **Forecasting House Prices:** Suppose you have data on house sizes (in square feet) and their selling prices. You can use linear regression to predict the selling price of a house based on its size.\n","\n","- **Estimating Gas Mileage:** If you have data on cars' engine sizes and their corresponding gas mileage, you can use linear regression to predict a car's gas mileage based on its engine size.\n","\n","## 2. Theory Behind Linear Regression"]},{"cell_type":"markdown","metadata":{},"source":["Recall the equation for a straight line from your early math classes,\n","\n","$$ y = mx + b$$\n","\n","The equation represents a straight line where $m$ is the slope and $b$ is the y-intercept.\n","\n","Here's a Python code example using matplotlib to plot the line represented by the equation $ y = mx + b$ and allowing you to adjust the values of $m$ and $b$."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from ipywidgets import interact, FloatSlider, Checkbox"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab48dfdd529d45c4890171ca5b29fcbb","version_major":2,"version_minor":0},"text/plain":["interactive(children=(FloatSlider(value=0.0, description='Slope (m)', max=10.0, min=-10.0, step=1.0), FloatSliâ€¦"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<function __main__.plot_line(m, b)>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Function to plot the line\n","def plot_line(m, b):\n","    x_vals = np.linspace(0, 10, 100)\n","    y_vals = m * x_vals + b\n","    plt.plot(x_vals, y_vals, color='red', label=f'y = {m:.2f}x + {b:.2f}')\n","    plt.xlabel('X')\n","    plt.ylabel('y')\n","    plt.title('y = mx + b')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","# Define sliders for m and b\n","m_slider = FloatSlider(min=-10, max=10, step=1, value=0, description='Slope (m)')\n","b_slider = FloatSlider(min=-10, max=10, step=1, value=0, description='Intercept (b)')\n","\n","# Create interactive plot\n","interact(plot_line, m=m_slider, b=b_slider)"]},{"cell_type":"markdown","metadata":{},"source":["Now imagine we have data on house sizes (in square feet) and their selling prices. Below, we generate some synthetic data for house sizes and selling prices."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>House Size (sqft)</th>\n","      <th>Selling Price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1684</td>\n","      <td>58670.101842</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1559</td>\n","      <td>84486.185954</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2653</td>\n","      <td>141294.361989</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2216</td>\n","      <td>103378.349796</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1835</td>\n","      <td>114447.546240</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1763</td>\n","      <td>73606.343254</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2731</td>\n","      <td>137007.585173</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2383</td>\n","      <td>117278.161500</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2033</td>\n","      <td>116977.792144</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2747</td>\n","      <td>152043.587699</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1277</td>\n","      <td>65399.474257</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2778</td>\n","      <td>142681.625196</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2828</td>\n","      <td>132522.142524</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1599</td>\n","      <td>60142.035318</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2094</td>\n","      <td>101220.878507</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2496</td>\n","      <td>126363.489691</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1600</td>\n","      <td>92302.906807</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2420</td>\n","      <td>133023.798488</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1314</td>\n","      <td>61826.731826</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1705</td>\n","      <td>82226.972494</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2510</td>\n","      <td>115014.470349</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>1551</td>\n","      <td>63349.820628</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>1087</td>\n","      <td>37287.298094</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>1174</td>\n","      <td>78207.753952</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2624</td>\n","      <td>126103.478182</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>1849</td>\n","      <td>88069.256984</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>2701</td>\n","      <td>122522.046400</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>1537</td>\n","      <td>84624.903558</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>1845</td>\n","      <td>76111.021524</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>1072</td>\n","      <td>51472.597198</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>1777</td>\n","      <td>79895.334388</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>2940</td>\n","      <td>150869.024979</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>1115</td>\n","      <td>50641.948624</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>1976</td>\n","      <td>86993.678159</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>1755</td>\n","      <td>87468.177717</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>2733</td>\n","      <td>140933.318705</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>2871</td>\n","      <td>144215.172224</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>2455</td>\n","      <td>125774.718977</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>1448</td>\n","      <td>66056.779063</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>1850</td>\n","      <td>88872.588340</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>1099</td>\n","      <td>48225.395522</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>2201</td>\n","      <td>106454.468385</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>1755</td>\n","      <td>79618.537180</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>1797</td>\n","      <td>72587.173977</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>1659</td>\n","      <td>84724.261423</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>2171</td>\n","      <td>104532.190638</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>1910</td>\n","      <td>79198.016530</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>1423</td>\n","      <td>75777.822555</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>2312</td>\n","      <td>106527.016356</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>2985</td>\n","      <td>149769.453958</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    House Size (sqft)  Selling Price\n","0                1684   58670.101842\n","1                1559   84486.185954\n","2                2653  141294.361989\n","3                2216  103378.349796\n","4                1835  114447.546240\n","5                1763   73606.343254\n","6                2731  137007.585173\n","7                2383  117278.161500\n","8                2033  116977.792144\n","9                2747  152043.587699\n","10               1277   65399.474257\n","11               2778  142681.625196\n","12               2828  132522.142524\n","13               1599   60142.035318\n","14               2094  101220.878507\n","15               2496  126363.489691\n","16               1600   92302.906807\n","17               2420  133023.798488\n","18               1314   61826.731826\n","19               1705   82226.972494\n","20               2510  115014.470349\n","21               1551   63349.820628\n","22               1087   37287.298094\n","23               1174   78207.753952\n","24               2624  126103.478182\n","25               1849   88069.256984\n","26               2701  122522.046400\n","27               1537   84624.903558\n","28               1845   76111.021524\n","29               1072   51472.597198\n","30               1777   79895.334388\n","31               2940  150869.024979\n","32               1115   50641.948624\n","33               1976   86993.678159\n","34               1755   87468.177717\n","35               2733  140933.318705\n","36               2871  144215.172224\n","37               2455  125774.718977\n","38               1448   66056.779063\n","39               1850   88872.588340\n","40               1099   48225.395522\n","41               2201  106454.468385\n","42               1755   79618.537180\n","43               1797   72587.173977\n","44               1659   84724.261423\n","45               2171  104532.190638\n","46               1910   79198.016530\n","47               1423   75777.822555\n","48               2312  106527.016356\n","49               2985  149769.453958"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Generate some random data for house sizes and selling prices\n","np.random.seed(0)\n","house_sizes = np.random.randint(1000, 3000, 50)  # House sizes in square feet\n","prices = 50 * house_sizes + np.random.normal(0, 10000, 50)  # Selling prices\n","\n","# Create a DataFrame\n","df = pd.DataFrame({'House Size (sqft)': house_sizes, 'Selling Price': prices})\n","\n","# Display the DataFrame\n","df"]},{"cell_type":"markdown","metadata":{},"source":["Now let's plot these data points in a scatter plot."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5be80a301304c65a9ca6915cf96f2a8","version_major":2,"version_minor":0},"text/plain":["interactive(children=(Checkbox(value=False, description='Show Lines'), Checkbox(value=False, description='Showâ€¦"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<function __main__.update_plot(show_lines, show_legend)>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Function to plot the data and multiple lines with different fits\n","def plot_data_and_lines(show_lines, show_legend):\n","    plt.figure(figsize=(10, 6))\n","    plt.scatter(house_sizes, prices, color='blue', label='Data')\n","\n","    # Plot multiple lines with different fits\n","    if show_lines:\n","        for i in range(-50, 51, 25):  # Generate 5 lines with different fits\n","            m = i\n","            b = np.mean(prices) - m * np.mean(house_sizes)  # Calculate intercept\n","            x_vals = np.linspace(min(house_sizes), max(house_sizes), 100)\n","            y_vals = m * x_vals + b\n","            plt.plot(x_vals, y_vals, label=f'y = {m:.2f}x + {b:.2f}')\n","\n","    # Show legend if specified\n","    if show_legend:\n","        plt.legend()\n","\n","    plt.xlabel('House Size (sqft)')\n","    plt.ylabel('Selling Price')\n","    plt.title('House Size vs Selling Price')\n","    plt.grid(True)\n","    plt.show()\n","\n","# Checkbox widget to show/hide the lines\n","lines_checkbox = Checkbox(value=False, description='Show Lines')  # Default value set to False\n","\n","# Checkbox widget to show/hide the legend\n","legend_checkbox = Checkbox(value=False, description='Show Legend')  # Default value set to False\n","\n","# Function to update the plot when checkboxes are toggled\n","def update_plot(show_lines, show_legend):\n","    plot_data_and_lines(show_lines, show_legend)\n","\n","# Create interactive plot with checkboxes\n","interact(update_plot, show_lines=lines_checkbox, show_legend=legend_checkbox)"]},{"cell_type":"markdown","metadata":{},"source":["Let's imagine 5 data scientists are working with the same dataset. If each scientist draws a different line of fit, how do they decide which line is best?\n","\n","How can we find a simple linear equation that best represents the relationship between the dependent variable, *price*, and the independent variable, *size*? In other words, how do we find the **line of best fit**?"]},{"cell_type":"markdown","metadata":{},"source":["### Line of Best Fit:\n","The line of best fit represents the linear relationship between the independent variable (predictor) and the dependent variable (response). In our case, the independent variable is the house size (size) and the dependent variable is the selling price (price). \n","\n","This line is often determined through linear regression, which aims to minimize **the difference between the observed values and the values predicted by the line**.\n","\n","### What are Residuals?\n","Residuals, denoted as $Îµ$ (epsilon), are the differences between the observed values ($y$) and the values predicted by the model ($\\hat{y}$). In other words, they represent the error in the model's predictions. Mathematically, residuals can be expressed as,\n","\n","$$Îµ_{i} = y_{i} - \\hat{y}_{i}$$\n","$$~~~~~~~~~~~~~~~~~~= y_{i} - (mx_{i} + b)$$\n","\n","where:\n","- $Îµ_{i}$ is the error or residual for the $i$ th data point\n","\n","- $y_{i}$ is the observed (actual) value for the $i$ th data point\n","\n","- $\\hat{y}_{i}$ is the predicted value by the model for the $i$ th data point\n","\n","- $m$ represents the slope of the line in a linear regression model\n","\n","- $x_{i}$ represents the value of the independent variable for the $i$ th data point\n","\n","- $b$ represents the y-intercept of the line in a linear regression model\n","\n","A residual is a measure of how well a line fits an individual data point. Consider this simple data set with a line of fit drawn through it.\n","\n","<p align=\"center\">\n","  <img src=\"/workspaces/themarisolhernandez-4geeks-ds-lessons/imgs/residual1.png\" alt=\"Alt text\" width=\"400\" height=\"400\">\n","</p>\n","\n","and notice how point **(2, 8)** is **<span style=\"color:green\">4</span>** units above the line:\n","\n","<p align=\"center\">\n","  <img src=\"/workspaces/themarisolhernandez-4geeks-ds-lessons/imgs/residual2.png\" alt=\"Alt text\" width=\"400\" height=\"400\">\n","</p>\n","\n","This vertical distance is known as a **residual**. For data points above the line, the residual is positive, and for data points below the line, the residual is negative.\n","\n","For example, the residual for the point **(4, 3)** is **<span style=\"color:red\">-2</span>**.\n","\n","<p align=\"center\">\n","  <img src=\"/workspaces/themarisolhernandez-4geeks-ds-lessons/imgs/residual3.png\" alt=\"Alt text\" width=\"400\" height=\"400\">\n","</p>\n","\n","The closer a data point's residual is to 0 the better the fit. In this case, the line fits the point (4, 3) better than (2, 8)."]},{"cell_type":"markdown","metadata":{},"source":["### Visualizing Residuals\n","We can further explore residuals by visualizing how they relate to our linear regression model for our housing dataset. Here, we can adjust the slope ($m$) and intercept ($b$) of the regression line and observe the corresponding residuals."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59ec721eed6f48aa8aee065fdfc297ed","version_major":2,"version_minor":0},"text/plain":["interactive(children=(Checkbox(value=False, description='Show Line'), Checkbox(value=False, description='Show â€¦"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<function __main__.update_plot(show_line, show_residuals, m, b)>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Function to plot the data, line, and residuals\n","def plot_data_line_residuals(m, b, show_line, show_residuals):\n","    plt.figure(figsize=(12, 6))\n","    plt.scatter(house_sizes, prices, color='blue', label='Data')\n","\n","    # Calculate predicted prices using the selected m and b\n","    predicted_prices = m * house_sizes + b\n","    \n","    # Plot the line if show_line is True\n","    if show_line:\n","        plt.plot(house_sizes, predicted_prices, color='red', label=f'y = {m}x + {b}')\n","\n","    # Plot dashed lines representing residuals if show_residuals is True\n","    if show_residuals:\n","        for i in range(len(house_sizes)):\n","            plt.plot([house_sizes[i], house_sizes[i]], [prices[i], predicted_prices[i]], color='green', linestyle='--', linewidth=0.8)\n","\n","        # Add legend for residuals if not already added\n","        handles, labels = plt.gca().get_legend_handles_labels()\n","        if 'Residuals' not in labels:\n","            plt.plot([], [], color='green', linestyle='--', label='Residuals')\n","\n","    plt.xlabel('House Size (sqft)')\n","    plt.ylabel('Selling Price')\n","    plt.title('House Size vs Selling Price')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","# Define sliders for m and b\n","m_slider = FloatSlider(min=-100, max=100, step=1, value=0, description='Slope (m)')\n","b_slider = FloatSlider(min=-50000, max=50000, step=1000, value=0, description='Intercept (b)')\n","\n","# Checkbox widget to show/hide the line\n","line_checkbox = Checkbox(value=False, description='Show Line')  # Default value set to False\n","\n","# Checkbox widget to show/hide the residuals\n","residuals_checkbox = Checkbox(value=False, description='Show Residuals')  # Default value set to False\n","\n","# Function to update the plot when checkboxes or sliders are adjusted\n","def update_plot(show_line, show_residuals, m, b):\n","    plot_data_line_residuals(m, b, show_line, show_residuals)\n","\n","# Create interactive plot with sliders and checkboxes\n","interact(update_plot, show_line=line_checkbox, show_residuals=residuals_checkbox, m=m_slider, b=b_slider)\n"]},{"cell_type":"markdown","metadata":{},"source":["So how do we know we've found the model parameters ($m$ and $b$) for the **line of best fit**?"]},{"cell_type":"markdown","metadata":{},"source":["### Determining Model Parameters for the Line of Best Fit\n","#### Method of Least Squares\n","In linear regression, the model parameters for the line of best fit are determined using the **method of least squares**. This method aims to <u>minimize</u> the sum of the squared differences between the observed values and the values predicted by the regression line.\n","\n","#### Mathematical Formulation\n","Given a set of $n$ data points $(x_{i}, y_{i})$, where $x_{i}$ represents the independent variable and $y_{i}$ represents the corresponding dependent variable, the line of best fit is represented by the equation:\n","\n","$$ \\hat{y}_{i} = mx_{i} + b$$\n","\n","where:\n","- $m$ is the slope of the line (coefficient for the independent variable $x$)\n","\n","- $x_{i}$ represents the value of the independent variable for the $i$-th data point\n","\n","- $b$ is the y-intercept of the line\n","\n","The goal is to find the values of $m$ and $b$ that minimize the **sum of the squared errors**, denoted as $SSE$:\n","\n","$$SSE = \\sum_{i=1}^{n}Îµ_{i}^2 = \\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^2 = \\sum_{i=1}^{n}(y_{i} - (mx_{i} + b))^2$$\n","\n","where:\n","- $\\hat{y}_{i}$ is the predicted value of $y_{i}$ at the $i$ th data point\n","\n","With our same housing dataset, we can adjust the slope ($m$) and intercept ($b$) of the regression line and observe the corresponding sum of squared errors."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50007901059845b5958bac3c67e14387","version_major":2,"version_minor":0},"text/plain":["interactive(children=(Checkbox(value=False, description='Show Line'), Checkbox(value=False, description='Show â€¦"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<function __main__.update_plot(show_line, show_residuals, m, b)>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Function to plot the data, line, residuals, and SSE\n","def plot_data_line_residuals_sse(m, b, show_line, show_residuals):\n","    plt.figure(figsize=(12, 6))\n","    plt.scatter(house_sizes, prices, color='blue', label='Data')\n","\n","    # Calculate predicted prices using the selected m and b\n","    predicted_prices = m * house_sizes + b\n","    \n","    # Calculate residuals\n","    residuals = prices - predicted_prices\n","    \n","    # Calculate sum of squared residuals if the line is shown\n","    if show_line:\n","        sum_squared_residuals = np.sum(residuals**2)\n","    else:\n","        sum_squared_residuals = None\n","    \n","    # Plot the line if show_line is True\n","    if show_line:\n","        plt.plot(house_sizes, predicted_prices, color='red', label=f'y = {m}x + {b}')\n","\n","    # Plot dashed lines representing residuals if show_residuals is True\n","    if show_residuals:\n","        for i in range(len(house_sizes)):\n","            plt.plot([house_sizes[i], house_sizes[i]], [prices[i], predicted_prices[i]], color='green', linestyle='--', linewidth=0.8)\n","\n","        # Add legend for residuals if not already added\n","        handles, labels = plt.gca().get_legend_handles_labels()\n","        if 'Residuals' not in labels:\n","            plt.plot([], [], color='green', linestyle='--', label='Residuals')\n","\n","    plt.xlabel('House Size (sqft)')\n","    plt.ylabel('Selling Price')\n","    title = 'House Size vs Selling Price'\n","    if show_line:\n","        title += f'\\nSum of Squared Errors: {sum_squared_residuals:.2f}'\n","    plt.title(title)\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","# Define sliders for m and b\n","m_slider = FloatSlider(min=-100, max=100, step=1, value=0, description='Slope (m)')\n","b_slider = FloatSlider(min=-50000, max=50000, step=1000, value=0, description='Intercept (b)')\n","\n","# Checkbox widget to show/hide the line\n","line_checkbox = Checkbox(value=False, description='Show Line')  # Default value set to False\n","\n","# Checkbox widget to show/hide the residuals\n","residuals_checkbox = Checkbox(value=False, description='Show Residuals')  # Default value set to False\n","\n","# Function to update the plot when checkboxes or sliders are adjusted\n","def update_plot(show_line, show_residuals, m, b):\n","    plot_data_line_residuals_sse(m, b, show_line, show_residuals)\n","\n","# Create interactive plot with sliders and checkboxes\n","interact(update_plot, show_line=line_checkbox, show_residuals=residuals_checkbox, m=m_slider, b=b_slider)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Introduction to Cost Function\n","Now that we've discussed the method of least squares and the goal of minimizing the sum of squared errors ($SSE$) to find the coefficients for the line of best fit, let's delve deeper into the concept of the **cost function**.\n","\n","In the realm of machine learning and optimization algorithms, a **cost function** serves as a critical component in evaluating the performance of a model. Also known as a loss function or objective function, it quantifies how well the model's predictions align with the actual observed values in the training dataset.\n","\n","##### Purpose of Cost Function\n","The primary purpose of a cost function is twofold:\n","\n","1. **Evaluation of Model Performance**: By assessing the extent of error or deviation between the predicted and actual values, the cost function provides insights into the efficacy of the model in capturing the underlying patterns and relationships within the data. A lower cost indicates better alignment between predictions and observations, signifying higher model accuracy.\n","\n","2. **Optimization**: Beyond evaluation, the cost function plays a pivotal role in the optimization process, guiding the iterative adjustment of model parameters to minimize prediction errors. Optimization algorithms, such as gradient descent, leverage the gradient (partial derivatives) of the cost function with respect to the model parameters to iteratively update the parameters and converge towards the optimal solution. More on this later...\n","\n","#### Cost Function: Sum of Squared Errors ($SSE$)\n","As we've seen earlier, the sum of squared errors ($SSE$) serves as a measure of the discrepancy between the observed values and the values predicted by our regression line. While $SSE$ is effective in quantifying the overall error, it has some limitations.\n","\n","#### Limitations of $SSE$\n","Although $SSE$ provides valuable insight into the model's performance, it does not account for the number of data points in the dataset. As a result, $SSE$ may vary significantly depending on the size of the dataset, making it challenging to compare models trained on different datasets directly.\n","\n","#### Introducing Mean Squared Error ($MSE$)\n","To address the limitations of $SSE$, we introduce the concept of **Mean Squared Error** ($MSE$). $MSE$ is obtained by dividing the $SSE$ by the number of data points, resulting in the average squared error per data point. Mathematically, it is written as:\n","\n","$$ MSE = \\frac{1}{n}SSE = \\frac{1}{n}\\sum_{i=1}^{n}Îµ_{i}^2 = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^2 = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - (mx_{i} + b))^2$$\n","\n","This normalization ensures that the cost function is independent of the dataset size, ensuring that the loss function is consistent across datasets of varying sizes. Additionally, $MSE$ provides a more intuitive measure of the model's performance, representing the average squared difference between the predicted and actual values.\n","\n","With our same housing dataset, we can adjust the slope ($m$) and intercept ($b$) of the regression line and observe the corresponding mean squared error ($MSE$)."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee5feaf66a7346aab35afc0574e67c65","version_major":2,"version_minor":0},"text/plain":["interactive(children=(Checkbox(value=False, description='Show Line'), Checkbox(value=False, description='Show â€¦"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<function __main__.update_plot(show_line, show_residuals, m, b)>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Function to plot the data, line, residuals, and MSE\n","def plot_data_line_residuals_mse(m, b, show_line, show_residuals):\n","    plt.figure(figsize=(12, 6))\n","    plt.scatter(house_sizes, prices, color='blue', label='Data')\n","\n","    # Calculate predicted prices using the selected m and b\n","    predicted_prices = m * house_sizes + b\n","    \n","    # Calculate residuals\n","    residuals = prices - predicted_prices\n","    n = len(residuals)\n","    \n","    # Calculate mean sequared error if the line is shown\n","    if show_line:\n","        mean_squared_error = (1/n)*np.sum(residuals**2)\n","    else:\n","        mean_squared_error = None\n","    \n","    # Plot the line if show_line is True\n","    if show_line:\n","        plt.plot(house_sizes, predicted_prices, color='red', label=f'y = {m}x + {b}')\n","\n","    # Plot dashed lines representing residuals if show_residuals is True\n","    if show_residuals:\n","        for i in range(len(house_sizes)):\n","            plt.plot([house_sizes[i], house_sizes[i]], [prices[i], predicted_prices[i]], color='green', linestyle='--', linewidth=0.8)\n","\n","        # Add legend for residuals if not already added\n","        handles, labels = plt.gca().get_legend_handles_labels()\n","        if 'Residuals' not in labels:\n","            plt.plot([], [], color='green', linestyle='--', label='Residuals')\n","\n","    plt.xlabel('House Size (sqft)')\n","    plt.ylabel('Selling Price')\n","    title = 'House Size vs Selling Price'\n","    if show_line:\n","        title += f'\\nMean Squared Error: {mean_squared_error:.2f}'\n","    plt.title(title)\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","# Define sliders for m and b\n","m_slider = FloatSlider(min=-100, max=100, step=1, value=0, description='Slope (m)')\n","b_slider = FloatSlider(min=-50000, max=50000, step=1000, value=0, description='Intercept (b)')\n","\n","# Checkbox widget to show/hide the line\n","line_checkbox = Checkbox(value=False, description='Show Line')  # Default value set to False\n","\n","# Checkbox widget to show/hide the residuals\n","residuals_checkbox = Checkbox(value=False, description='Show Residuals')  # Default value set to False\n","\n","# Function to update the plot when checkboxes or sliders are adjusted\n","def update_plot(show_line, show_residuals, m, b):\n","    plot_data_line_residuals_mse(m, b, show_line, show_residuals)\n","\n","# Create interactive plot with sliders and checkboxes\n","interact(update_plot, show_line=line_checkbox, show_residuals=residuals_checkbox, m=m_slider, b=b_slider)"]},{"cell_type":"markdown","metadata":{},"source":["#### Gradient Descent\n","Now that we've established $MSE$ as our preferred cost function, let's explore how we can optimize our linear regression model using **gradient descent**. Gradient descent is an iterative optimization algorithm that aims to <u>minimize</u> the cost function ($MSE$) by adjusting the model parameters (slope and intercept).\n","\n","##### Mathematics Behind Gradient Descent \n","Up to this point, we've been using $\\hat{y}_{i}$ to represent the predicted value for the $i$-th training example,\n","\n","$$ \\hat{y}_{i} = mx_{i} + b$$\n","\n","where:\n","- $\\hat{y}_{i}$ is the predicted value for the $i$-th data point\n","\n","- $m$ is the slope of the line (coefficient for the independent variable $x$)\n","\n","- $x_{i}$ represents the value of the independent variable for the $i$-th data point\n","\n","- $b$ is the y-intercept of the line\n","\n","We can also express the linear regression with the following notation,\n","\n","$$h_{\\theta}(x_{i}) = \\theta_{0} + \\theta_{1}x_{i}$$\n","\n","where:\n","\n","- $h_{\\theta}(x_{i})$ is the predicted value for the $i$-th data point\n","\n","- $\\theta_{0}$ corresponds to the y-intercept ($b$)\n","\n","- $\\theta_{1}$ corresponds to the slope ($m$)\n","\n","- $x_{i}$ represents the value of the independent variable for the $i$-th data point\n","\n","To minimize the cost function ($MSE$), the model needs to find the best value of $\\theta_{0}$ and $\\theta_{1}$. We will find the optimal values for $\\theta_{0}$ and $\\theta_{1}$ using gradient descent in a step-by-step process.\n","\n","##### Step-by-Step Process of Gradient Descent\n","1. **Initialization**: We initialize the values of $\\theta_{0}$ and $\\theta_{1}$ to some random values or zeros. These values represent the parameters of the linear regression model.\n","\n","2. **Define the Cost Function:** We define the Mean Squared Error ($MSE$) as our cost function. The $MSE$ represents the average squared difference between the predicted and actual values over all data points. We can rewrite our cost function in terms of $h_{\\theta}(x_{i})$,\n","\n","$$ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^2 = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - h_{\\theta}(x_{i}))^2 $$\n","\n","We can actually scale our cost function using a factor of $\\frac{1}{2n}$ so that we get a cost function the looks like,\n","\n","$$ J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^{n}(h_{\\theta}(x_{i}) - y_{i})^2$$\n","\n","The factor $\\frac{1}{2}$ is often included for mathematical convenience as it simplifies the derivative computation in the next step. The additional $\\frac{1}{n}$ factor adjusts the scaling based on the number of training examples, making the cost function more consistent across datasets of different sizes. \n","\n","The subtraction $h_{\\theta}(x_{i}) - y_{i}$ is just a rearrangement; it serves the same purpose of quantifying the discrepancy between predicted and actual values, ultimately resulting in the same optimization goal of minimizing the cost function. For example, $(3 - 1)^2 = 2^2 = 4$ which is the same as  $(1 - 3)^2 = (-2)^2 = 4$\n","\n","3. **Compute the Gradient:** Compute the gradient of the cost function with respect to each parameter. It involves making partial differentiation of cost function with respect to the parameters.\n","\n","Partial derivative with respect to $\\theta_0$ simplifies to,\n","\n","$$\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{n}\\sum_{i=1}^{n}(h_{\\theta}(x_{i}) - y_{i})$$ \n","\n","Partial derivative with respect to $\\theta_1$ simplifies to,\n","\n","$$\\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{1}{n}\\sum_{i=1}^{n}(h_{\\theta}(x_{i}) - y_{i}) \\cdot x_{i}$$ \n","\n","4. **Set the Learning Rate:** We choose a learning rate, denoted as $\\alpha$, which determines the size of the steps we take in the direction of the gradient.\n","\n","5. **Update the Parameters:** Using the gradient and the learning rate, we update the parameters iteratively. The update rule for each parameter is:\n","\n","$$\\theta_{0} = \\theta_{0} - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_0}$$\n","\n","$$\\theta_{1} = \\theta_{1} - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_1}$$\n","\n","We apply this update rule to each coefficient, moving them in the direction that reduces the cost function.\n","\n","6. **Repeat Until Convergence:** We repeat steps 3 through 5 until the cost function converges to a minimum. Convergence is typically checked by monitoring the change in the cost function or after a predetermined number of iterations.\n","\n","7. **Final Parameters:** Once the algorithm converges, the final parameters $\\theta_0$ and $\\theta_1$ represent the best-fit line for our linear regression model."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1: Theta0 = 0.00973759559053377, Theta1 = 21.070174748626858, Cost = 1732085737.1576931\n","Iteration 2: Theta0 = 0.015265495713580827, Theta1 = 33.08447970137416, Cost = 598841366.9699563\n","Iteration 3: Theta0 = 0.018393009240108678, Theta1 = 39.935088456303106, Cost = 230386246.0854429\n","Iteration 4: Theta0 = 0.020151811928302902, Theta1 = 43.84133526429951, Cost = 110589334.05637309\n","Iteration 5: Theta0 = 0.021130169765860025, Theta1 = 46.06869413186921, Cost = 71639406.85028514\n","Iteration 6: Theta0 = 0.021663514567994143, Theta1 = 47.33874382097777, Cost = 58975500.8660717\n","Iteration 7: Theta0 = 0.021943111009208028, Theta1 = 48.0629316683883, Cost = 54858047.33385395\n","Iteration 8: Theta0 = 0.022078019035676726, Theta1 = 48.47586672837551, Cost = 53519327.36871705\n","Iteration 9: Theta0 = 0.02213042510127954, Theta1 = 48.711324100276975, Cost = 53084065.333076894\n","Iteration 10: Theta0 = 0.022135788191480064, Theta1 = 48.84558292539911, Cost = 52942547.26772601\n","Iteration 11: Theta0 = 0.022114327173437826, Theta1 = 48.92213790064179, Cost = 52896535.055236906\n","Iteration 12: Theta0 = 0.022077570932818053, Theta1 = 48.965789890645276, Cost = 52881574.94544073\n","Iteration 13: Theta0 = 0.02203209329018315, Theta1 = 48.990680455892644, Cost = 52876710.89837541\n","Iteration 14: Theta0 = 0.02198164266650298, Theta1 = 49.004873175448495, Cost = 52875129.414287284\n","Iteration 15: Theta0 = 0.02192835642765693, Theta1 = 49.01296593703595, Cost = 52874615.199686944\n","Iteration 16: Theta0 = 0.0218734533089309, Theta1 = 49.01758047615795, Cost = 52874447.989597276\n","Iteration 17: Theta0 = 0.02181762823846584, Theta1 = 49.02021171775858, Cost = 52874393.60211873\n","Iteration 18: Theta0 = 0.02176127746729893, Theta1 = 49.021712074518646, Cost = 52874375.89699215\n","Iteration 19: Theta0 = 0.02170462693948858, Theta1 = 49.02256759584236, Cost = 52874370.11850799\n","Iteration 20: Theta0 = 0.021647805489315927, Theta1 = 49.02305542920612, Cost = 52874368.21776869\n","Iteration 21: Theta0 = 0.021590886578646343, Theta1 = 49.023333605301865, Cost = 52874367.57781211\n","Iteration 22: Theta0 = 0.021533912095753775, Theta1 = 49.02349223391492, Cost = 52874367.347777545\n","Iteration 23: Theta0 = 0.02147690592551354, Theta1 = 49.02358269603534, Cost = 52874367.25102222\n","Iteration 24: Theta0 = 0.021419881687185033, Theta1 = 49.02363428932628, Cost = 52874367.19760032\n","Iteration 25: Theta0 = 0.02136284714652898, Theta1 = 49.023663719442226, Cost = 52874367.15826756\n","Iteration 26: Theta0 = 0.021305806731615796, Theta1 = 49.02368051201975, Cost = 52874367.12351564\n","Iteration 27: Theta0 = 0.02124876296735042, Theta1 = 49.02369009861812, Cost = 52874367.090253115\n","Iteration 28: Theta0 = 0.021191717293444587, Theta1 = 49.02369557633594, Cost = 52874367.05747481\n","Iteration 29: Theta0 = 0.021134670530828167, Theta1 = 49.023698711152264, Cost = 52874367.024853975\n","Iteration 30: Theta0 = 0.02107762314759893, Theta1 = 49.02370051003601, Cost = 52874366.992284335\n","Iteration 31: Theta0 = 0.021020575410668253, Theta1 = 49.02370154716521, Cost = 52874366.95973135\n","Iteration 32: Theta0 = 0.020963527472230157, Theta1 = 49.023702149938664, Cost = 52874366.92718375\n","Iteration 33: Theta0 = 0.020906479419066336, Theta1 = 49.023702505040596, Cost = 52874366.89463791\n","Iteration 34: Theta0 = 0.020849431300660085, Theta1 = 49.02370271891915, Cost = 52874366.86209263\n","Iteration 35: Theta0 = 0.020792383145226995, Theta1 = 49.02370285227151, Cost = 52874366.82954757\n","Iteration 36: Theta0 = 0.02073533496885572, Theta1 = 49.0237029397075, Cost = 52874366.79700255\n","Iteration 37: Theta0 = 0.020678286780720073, Theta1 = 49.02370300096177, Cost = 52874366.76445756\n","Iteration 38: Theta0 = 0.02062123858605101, Theta1 = 49.023703047287114, Cost = 52874366.731912576\n","Iteration 39: Theta0 = 0.020564190387831242, Theta1 = 49.02370308509992, Cost = 52874366.6993676\n","Iteration 40: Theta0 = 0.020507142187761524, Theta1 = 49.02370311805883, Cost = 52874366.66682261\n","Iteration 41: Theta0 = 0.02045009398681163, Theta1 = 49.02370314825004, Cost = 52874366.63427762\n","Iteration 42: Theta0 = 0.020393045785534535, Theta1 = 49.02370317686309, Cost = 52874366.60173266\n","Iteration 43: Theta0 = 0.020335997584245546, Theta1 = 49.023703204576265, Cost = 52874366.56918769\n","Iteration 44: Theta0 = 0.02027894938312445, Theta1 = 49.023703231776324, Cost = 52874366.53664268\n","Iteration 45: Theta0 = 0.020221901182273766, Theta1 = 49.02370325868381, Cost = 52874366.50409772\n","Iteration 46: Theta0 = 0.020164852981751948, Theta1 = 49.02370328542446, Cost = 52874366.471552715\n","Iteration 47: Theta0 = 0.020107804781592326, Theta1 = 49.02370331206999, Cost = 52874366.43900776\n","Iteration 48: Theta0 = 0.02005075658181391, Theta1 = 49.02370333866128, Cost = 52874366.40646277\n","Iteration 49: Theta0 = 0.019993708382427533, Theta1 = 49.023703365221635, Cost = 52874366.3739178\n","Iteration 50: Theta0 = 0.019936660183439377, Theta1 = 49.02370339176436, Cost = 52874366.341372825\n","Iteration 51: Theta0 = 0.01987961198485296, Theta1 = 49.02370341829702, Cost = 52874366.30882785\n","Iteration 52: Theta0 = 0.0198225637866703, Theta1 = 49.02370344482395, Cost = 52874366.276282884\n","Iteration 53: Theta0 = 0.019765515588892536, Theta1 = 49.023703471347616, Cost = 52874366.243737906\n","Iteration 54: Theta0 = 0.019708467391520323, Theta1 = 49.02370349786941, Cost = 52874366.211192936\n","Iteration 55: Theta0 = 0.019651419194554036, Theta1 = 49.02370352439014, Cost = 52874366.17864793\n","Iteration 56: Theta0 = 0.019594370997993885, Theta1 = 49.02370355091027, Cost = 52874366.14610298\n","Iteration 57: Theta0 = 0.01953732280183999, Theta1 = 49.02370357743005, Cost = 52874366.113557994\n","Iteration 58: Theta0 = 0.01948027460609242, Theta1 = 49.023703603949635, Cost = 52874366.081013024\n","Iteration 59: Theta0 = 0.01942322641075122, Theta1 = 49.02370363046911, Cost = 52874366.04846807\n","Iteration 60: Theta0 = 0.019366178215816408, Theta1 = 49.023703656988516, Cost = 52874366.015923075\n","Iteration 61: Theta0 = 0.019309130021287993, Theta1 = 49.02370368350788, Cost = 52874365.9833781\n","Iteration 62: Theta0 = 0.01925208182716599, Theta1 = 49.023703710027235, Cost = 52874365.95083312\n","Iteration 63: Theta0 = 0.019195033633450397, Theta1 = 49.02370373654657, Cost = 52874365.91828816\n","Iteration 64: Theta0 = 0.01913798544014122, Theta1 = 49.0237037630659, Cost = 52874365.88574319\n","Iteration 65: Theta0 = 0.019080937247238458, Theta1 = 49.02370378958523, Cost = 52874365.853198215\n","Iteration 66: Theta0 = 0.019023889054742116, Theta1 = 49.02370381610455, Cost = 52874365.82065325\n","Iteration 67: Theta0 = 0.01896684086265219, Theta1 = 49.023703842623874, Cost = 52874365.78810829\n","Iteration 68: Theta0 = 0.01890979267096868, Theta1 = 49.0237038691432, Cost = 52874365.75556332\n","Iteration 69: Theta0 = 0.01885274447969159, Theta1 = 49.023703895662514, Cost = 52874365.72301834\n","Iteration 70: Theta0 = 0.018795696288820916, Theta1 = 49.02370392218183, Cost = 52874365.690473385\n","Iteration 71: Theta0 = 0.018738648098356663, Theta1 = 49.023703948701154, Cost = 52874365.65792842\n","Iteration 72: Theta0 = 0.01868159990829883, Theta1 = 49.02370397522048, Cost = 52874365.625383444\n","Iteration 73: Theta0 = 0.01862455171864741, Theta1 = 49.023704001739794, Cost = 52874365.59283851\n","Iteration 74: Theta0 = 0.018567503529402406, Theta1 = 49.02370402825911, Cost = 52874365.560293525\n","Iteration 75: Theta0 = 0.018510455340563826, Theta1 = 49.023704054778435, Cost = 52874365.527748525\n","Iteration 76: Theta0 = 0.01845340715213166, Theta1 = 49.02370408129775, Cost = 52874365.49520357\n","Iteration 77: Theta0 = 0.018396358964105915, Theta1 = 49.02370410781707, Cost = 52874365.462658636\n","Iteration 78: Theta0 = 0.018339310776486588, Theta1 = 49.02370413433639, Cost = 52874365.43011365\n","Iteration 79: Theta0 = 0.01828226258927368, Theta1 = 49.02370416085571, Cost = 52874365.39756871\n","Iteration 80: Theta0 = 0.018225214402467188, Theta1 = 49.023704187375024, Cost = 52874365.36502374\n","Iteration 81: Theta0 = 0.018168166216067115, Theta1 = 49.02370421389434, Cost = 52874365.33247877\n","Iteration 82: Theta0 = 0.01811111803007346, Theta1 = 49.02370424041366, Cost = 52874365.29993381\n","Iteration 83: Theta0 = 0.018054069844486223, Theta1 = 49.02370426693298, Cost = 52874365.26738888\n","Iteration 84: Theta0 = 0.017997021659305404, Theta1 = 49.0237042934523, Cost = 52874365.23484387\n","Iteration 85: Theta0 = 0.017939973474531003, Theta1 = 49.023704319971614, Cost = 52874365.20229891\n","Iteration 86: Theta0 = 0.01788292529016302, Theta1 = 49.02370434649093, Cost = 52874365.16975396\n","Iteration 87: Theta0 = 0.017825877106201454, Theta1 = 49.02370437301025, Cost = 52874365.13720899\n","Iteration 88: Theta0 = 0.017768828922646307, Theta1 = 49.023704399529564, Cost = 52874365.10466404\n","Iteration 89: Theta0 = 0.017711780739497578, Theta1 = 49.02370442604888, Cost = 52874365.07211909\n","Iteration 90: Theta0 = 0.017654732556755266, Theta1 = 49.0237044525682, Cost = 52874365.03957411\n","Iteration 91: Theta0 = 0.017597684374419373, Theta1 = 49.02370447908751, Cost = 52874365.007029176\n","Iteration 92: Theta0 = 0.017540636192489897, Theta1 = 49.02370450560683, Cost = 52874364.97448421\n","Iteration 93: Theta0 = 0.01748358801096684, Theta1 = 49.023704532126146, Cost = 52874364.94193926\n","Iteration 94: Theta0 = 0.0174265398298502, Theta1 = 49.02370455864546, Cost = 52874364.90939429\n","Iteration 95: Theta0 = 0.01736949164913998, Theta1 = 49.02370458516478, Cost = 52874364.87684934\n","Iteration 96: Theta0 = 0.017312443468836175, Theta1 = 49.023704611684096, Cost = 52874364.844304375\n","Iteration 97: Theta0 = 0.01725539528893879, Theta1 = 49.02370463820341, Cost = 52874364.81175943\n","Iteration 98: Theta0 = 0.01719834710944782, Theta1 = 49.02370466472273, Cost = 52874364.77921448\n","Iteration 99: Theta0 = 0.01714129893036327, Theta1 = 49.023704691242045, Cost = 52874364.746669516\n","Iteration 100: Theta0 = 0.017084250751685133, Theta1 = 49.023704717761355, Cost = 52874364.714124575\n","\n","Final Parameters:\n","Theta0 = 0.017084250751685133, Theta1 = 49.023704717761355\n"]}],"source":["# Initialize parameters theta0 and theta1\n","theta0 = 0\n","theta1 = 0\n","\n","# Define the number of iterations and learning rate\n","num_iterations = 100\n","learning_rate = 0.0000001 \n","\n","# Define the cost function\n","def cost_function(theta0, theta1, x, y):\n","    predictions = theta0 + theta1 * x\n","    errors = predictions - y\n","    n = len(x)\n","    cost = (1 / (2 * n)) * np.sum(errors ** 2)\n","    return cost\n","\n","# Compute the gradient of the cost function\n","def compute_gradient(theta0, theta1, x, y):\n","    predictions = theta0 + theta1 * x\n","    errors = predictions - y\n","    n = len(x)\n","    gradient_theta0 = (1 / n) * np.sum(errors)\n","    gradient_theta1 = (1 / n) * np.sum(errors * x)\n","    return gradient_theta0, gradient_theta1\n","\n","# Perform gradient descent\n","for i in range(num_iterations):\n","    # Compute the gradient\n","    gradient_theta0, gradient_theta1 = compute_gradient(theta0, theta1, house_sizes, prices)\n","    \n","    # Update the parameters\n","    theta0 -= learning_rate * gradient_theta0\n","    theta1 -= learning_rate * gradient_theta1\n","    \n","    # Compute the cost\n","    cost = cost_function(theta0, theta1, house_sizes, prices)\n","    \n","    # Display the updated parameters and cost\n","    print(f\"Iteration {i + 1}: Theta0 = {theta0}, Theta1 = {theta1}, Cost = {cost}\")\n","\n","# Final parameters\n","print(\"\\nFinal Parameters:\")\n","print(f\"Theta0 = {theta0}, Theta1 = {theta1}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
